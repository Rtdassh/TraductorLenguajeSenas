# ğŸŒ Language IA (LIA)

## ğŸ“„ Resumen
**LIA** es una aplicaciÃ³n web diseÃ±ada para facilitar la comunicaciÃ³n entre personas con discapacidad del habla o auditiva y aquellas que no conocen el lenguaje de seÃ±as.

## ğŸ¯ Objetivo
Desarrollar una herramienta accesible que permita la comunicaciÃ³n efectiva entre personas con discapacidades del habla/auditiva y sus interlocutores, eliminando barreras de comunicaciÃ³n y promoviendo la inclusiÃ³n.

## ğŸ“Š Contexto
SegÃºn la Encuesta Nacional sobre Discapacidad (ENDIS) de 2016, el 10.2% de la poblaciÃ³n posee alguna discapacidad, aunque las estadÃ­sticas especÃ­ficas para personas con discapacidad auditiva o del habla son limitadas. Este proyecto busca atender las necesidades de comunicaciÃ³n de este grupo, que enfrenta dificultades para integrarse plenamente en la sociedad debido a barreras lingÃ¼Ã­sticas.

La Universidad Rafael LandÃ­var actualmente cuenta con un Programa de Apoyo AcadÃ©mico que ofrece tutorÃ­as y apoyo personalizado a estudiantes con discapacidades, pero aÃºn no dispone de un programa de lengua de seÃ±as. **LIA** surge como una propuesta para mejorar la accesibilidad y fomentar la inclusiÃ³n.

## ğŸ› ï¸ DescripciÃ³n General del Programa
**LIA** permite traducir en tiempo real gestos de lenguaje de seÃ±as a texto, facilitando la comunicaciÃ³n. Esta funcionalidad se logra mediante el uso de tecnologÃ­as avanzadas de procesamiento de imÃ¡genes y redes neuronales.

## ğŸ” Estructura de LIA

1. **ğŸ¥ Captura y Procesamiento de ImÃ¡genes**
   - LibrerÃ­as utilizadas: **MediaPipe** y **OpenCV**.
   - **MediaPipe** identifica puntos clave de las manos en tiempo real, mientras que **OpenCV** facilita la captura de video y preprocesamiento de imÃ¡genes para extraer estos puntos.

2. **ğŸ”§ Preprocesamiento de Datos**
   - Los puntos clave capturados se normalizan y organizan en secuencias temporales para representar los movimientos del lenguaje de seÃ±as.
   - Herramientas: **NumPy**.

3. **ğŸ§  CreaciÃ³n y Entrenamiento de la Red Neuronal Recurrente (RNN)**
   - El modelo de RNN/LSTM se entrena usando **TensorFlow** y **Keras**, permitiendo que la IA asocie patrones de gestos con palabras o frases especÃ­ficas.

4. **âœ… ValidaciÃ³n del Modelo**
   - Para evaluar y ajustar el modelo, se utilizan **Scikit-learn** y **Matplotlib**, proporcionando mÃ©tricas de precisiÃ³n y visualizaciÃ³n de los resultados.

5. **ğŸŒ IntegraciÃ³n en la AplicaciÃ³n Web**
   - La cÃ¡mara del usuario captura gestos continuamente, que luego son procesados en un servidor y traducidos en tiempo real a texto en la interfaz de usuario.

## ğŸ’» TecnologÃ­as Utilizadas
- **Front-End**: HTML, JavaScript y CSS.
- **Procesamiento de ImÃ¡genes y IA**: MediaPipe, OpenCV, NumPy, TensorFlow, Keras, Scikit-learn, Matplotlib.

# ğŸ¤– GuÃ­a de Uso de Reconocimiento de Gestos con IA

## ğŸ” Instrucciones de Uso

## ğŸ’» InstalaciÃ³n de Dependencias
Debes tener Python y un gestor de paquetes instalado en tu dispositivo. Si no tienes Python, puedes instalarlo desde la Microsoft Store. Pip es el gestor de paquetes recomendado. Puedes ver este tutorial si tienes problemas https://youtu.be/fJKdIf11GcI?si=BHnzXWClLwlpM57Y 

Si estÃ¡s en Windows, asegÃºrate de habilitar las rutas de Win32 largas. Si tienes problemas ejecuta lo siguiente sigue las siguientes instrucciones. https://learn.microsoft.com/en-us/windows/win32/fileio/maximum-file-path-limitation?tabs=registry#enable-long-paths-in-windows-10-version-1607-and-later

Abre la terminal y dirÃ­gete al directorio del proyecto. Instala las librerÃ­as ejecutando el siguiente comando: 
- pip install -r requirements.txt

## ğŸ¥ ConfiguraciÃ³n de Captura de Imagen
Ingresa a recolecciÃ³n_datos.py e ingresa el Ã­ndice del dispositivo que utilices para captar imagen. Repite este paso en reconocimiento.py
- cv2.VideoCapture(Ã­ndice)

## âœ‹ Entrenamiento de Gestos
Escribe en recolecciÃ³n_datos.py el gesto que deseas entrenar y ejecuta el archivo. Puedes repetir este paso para mÃºltiples gestos:
- ingresar_gesto('nombre_del_gesto', 50, 60)

## ğŸ·ï¸ Etiquetado y Entrenamiento de Datos
Ejecuta etiquetado_datos.py para etiquetar las muestras.
Luego, ejecuta entrenamiento_datos.py para entrenar el modelo de reconocimiento de gestos.

## ğŸ” Reconocimiento de Gestos
Ejecuta reconocimiento.py para comenzar a reconocer y predecir los gestos entrenados en tiempo real.

## ğŸš€ Despliegue
La aplicaciÃ³n LIA estÃ¡ desplegada en una pÃ¡gina web que utiliza **HTML, JavaScript y CSS** para la interfaz, asegurando una experiencia interactiva y accesible para los usuarios.

## ğŸ¤ ContribuciÃ³n
Las contribuciones al desarrollo de LIA son bienvenidas. Sugerencias, mejoras y reportes de errores pueden realizarse a travÃ©s de GitHub.

---
